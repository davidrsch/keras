__signature__
keras.ops.lstsq(
  a,
  b,
  rcond=None
)
__doc__
Return the least-squares solution to a linear matrix equation.

Computes the vector x that approximately solves the equation
`a @ x = b`. The equation may be under-, well-, or over-determined
(i.e., the number of linearly independent rows of a can be less than,
equal to, or greater than its number of linearly independent columns).
If a is square and of full rank, then `x` (but for round-off error)
is the exact solution of the equation. Else, `x` minimizes the
L2 norm of `b - a * x`.

If there are multiple minimizing solutions,
the one with the smallest L2 norm  is returned.

Args:
    a: "Coefficient" matrix of shape `(M, N)`.
    b: Ordinate or "dependent variable" values,
        of shape `(M,)` or `(M, K)`.
        If `b` is two-dimensional, the least-squares solution
        is calculated for each of the K columns of `b`.
    rcond: Cut-off ratio for small singular values of `a`.
        For the purposes of rank determination,
        singular values are treated as zero if they are
        smaller than rcond times the largest
        singular value of `a`.

Returns:
    Tensor with shape `(N,)` or `(N, K)` containing
    the least-squares solutions.

**NOTE:** The output differs from `numpy.linalg.lstsq`.
NumPy returns a tuple with four elements, the first of which
being the least-squares solutions and the others
being essentially never used.
Keras only returns the first value. This is done both
to ensure consistency across backends (which cannot be achieved
for the other values) and to simplify the API.

