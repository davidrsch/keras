__signature__
keras.Model.export(
  self,
  filepath,
  format='tf_saved_model',
  verbose=None,
  input_signature=None,
  **kwargs
)
__doc__
Export the model as an artifact for inference.

Args:
    filepath: `str` or `pathlib.Path` object. The path to save the
        artifact.
    format: `str`. The export format. Supported values:
        `"tf_saved_model"` and `"onnx"`.  Defaults to
        `"tf_saved_model"`.
    verbose: `bool`. Whether to print a message during export. Defaults
        to `None`, which uses the default value set by different
        backends and formats.
    input_signature: Optional. Specifies the shape and dtype of the
        model inputs. Can be a structure of `keras.InputSpec`,
        `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If
        not provided, it will be automatically computed. Defaults to
        `None`.
    **kwargs: Additional keyword arguments:
        - Specific to the JAX backend and `format="tf_saved_model"`:
            - `is_static`: Optional `bool`. Indicates whether `fn` is
                static. Set to `False` if `fn` involves state updates
                (e.g., RNG seeds and counters).
            - `jax2tf_kwargs`: Optional `dict`. Arguments for
                `jax2tf.convert`. See the documentation for
                [`jax2tf.convert`](
                    https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).
                If `native_serialization` and `polymorphic_shapes` are
                not provided, they will be automatically computed.

**Note:** This feature is currently supported only with TensorFlow, JAX
and Torch backends.

**Note:** Be aware that the exported artifact may contain information
from the local file system when using `format="onnx"`, `verbose=True`
and Torch backend.

Examples:

Here's how to export a TensorFlow SavedModel for inference.

```python
# Export the model as a TensorFlow SavedModel artifact
model.export("path/to/location", format="tf_saved_model")

# Load the artifact in a different process/environment
reloaded_artifact = tf.saved_model.load("path/to/location")
predictions = reloaded_artifact.serve(input_data)
```

Here's how to export an ONNX for inference.

```python
# Export the model as a ONNX artifact
model.export("path/to/location", format="onnx")

# Load the artifact in a different process/environment
ort_session = onnxruntime.InferenceSession("path/to/location")
ort_inputs = {
    k.name: v for k, v in zip(ort_session.get_inputs(), input_data)
}
predictions = ort_session.run(None, ort_inputs)
```

