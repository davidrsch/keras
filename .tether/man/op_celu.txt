__signature__
keras.ops.celu(x, alpha=1.0)
__doc__
Continuously-differentiable exponential linear unit.

It is defined as:

`f(x) =  alpha * (exp(x / alpha) - 1) for x < 0`, `f(x) = x for x >= 0`.

Args:
    x: Input tensor.
    alpha: the Î± value for the CELU formulation. Defaults to `1.0`.

Returns:
    A tensor with the same shape as `x`.

Example:

>>> x = np.array([-1., 0., 1.])
>>> x_celu = keras.ops.celu(x)
>>> print(x_celu)
array([-0.63212056, 0. , 1. ], shape=(3,), dtype=float64)

