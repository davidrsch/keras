Help on class CategoricalCrossentropy in module keras.src.losses.losses:

class CategoricalCrossentropy(LossFunctionWrapper)
 |  CategoricalCrossentropy(from_logits=False, label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size', name='categorical_crossentropy', dtype=None)
 |
 |  Computes the crossentropy loss between the labels and predictions.
 |
 |  Use this crossentropy loss function when there are two or more label
 |  classes. We expect labels to be provided in a `one_hot` representation. If
 |  you want to provide labels as integers, please use
 |  `SparseCategoricalCrossentropy` loss. There should be `num_classes` floating
 |  point values per feature, i.e., the shape of both `y_pred` and `y_true` are
 |  `[batch_size, num_classes]`.
 |
 |  Args:
 |      from_logits: Whether `y_pred` is expected to be a logits tensor. By
 |          default, we assume that `y_pred` encodes a probability distribution.
 |      label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,
 |          meaning the confidence on label values are relaxed. For example, if
 |          `0.1`, use `0.1 / num_classes` for non-target labels and
 |          `0.9 + 0.1 / num_classes` for target labels.
 |      axis: The axis along which to compute crossentropy (the features
 |          axis). Defaults to `-1`.
 |      reduction: Type of reduction to apply to the loss. In almost all cases
 |          this should be `"sum_over_batch_size"`. Supported options are
 |          `"sum"`, `"sum_over_batch_size"`, `"mean"`,
 |          `"mean_with_sample_weight"` or `None`. `"sum"` sums the loss,
 |          `"sum_over_batch_size"` and `"mean"` sum the loss and divide by the
 |          sample size, and `"mean_with_sample_weight"` sums the loss and
 |          divides by the sum of the sample weights. `"none"` and `None`
 |          perform no aggregation. Defaults to `"sum_over_batch_size"`.
 |      name: Optional name for the loss instance.
 |      dtype: The dtype of the loss's computations. Defaults to `None`, which
 |          means using `keras.backend.floatx()`. `keras.backend.floatx()` is a
 |          `"float32"` unless set to different value
 |          (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is
 |          provided, then the `compute_dtype` will be utilized.
 |
 |  Examples:
 |
 |  Standalone usage:
 |
 |  >>> y_true = [[0, 1, 0], [0, 0, 1]]
 |  >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
 |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.
 |  >>> cce = keras.losses.CategoricalCrossentropy()
 |  >>> cce(y_true, y_pred)
 |  1.177
 |
 |  >>> # Calling with 'sample_weight'.
 |  >>> cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
 |  0.814
 |
 |  >>> # Using 'sum' reduction type.
 |  >>> cce = keras.losses.CategoricalCrossentropy(
 |  ...     reduction="sum")
 |  >>> cce(y_true, y_pred)
 |  2.354
 |
 |  >>> # Using 'none' reduction type.
 |  >>> cce = keras.losses.CategoricalCrossentropy(
 |  ...     reduction=None)
 |  >>> cce(y_true, y_pred)
 |  array([0.0513, 2.303], dtype=float32)
 |
 |  Usage with the `compile()` API:
 |
 |  ```python
 |  model.compile(optimizer='sgd',
 |                loss=keras.losses.CategoricalCrossentropy())
 |  ```
 |
 |  Method resolution order:
 |      CategoricalCrossentropy
 |      LossFunctionWrapper
 |      keras.src.losses.loss.Loss
 |      keras.src.saving.keras_saveable.KerasSaveable
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    from_logits=False,
 |    label_smoothing=0.0,
 |    axis=-1,
 |    reduction='sum_over_batch_size',
 |    name='categorical_crossentropy',
 |    dtype=None
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  get_config(self)
 |

