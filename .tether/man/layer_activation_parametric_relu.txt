Help on class PReLU in module keras.src.layers.activations.prelu:

class PReLU(keras.src.layers.layer.Layer)
 |  PReLU(alpha_initializer='Zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, **kwargs)
 |
 |  Parametric Rectified Linear Unit activation layer.
 |
 |  Formula:
 |  ``` python
 |  f(x) = alpha * x for x < 0
 |  f(x) = x for x >= 0
 |  ```
 |  where `alpha` is a learned array with the same shape as x.
 |
 |  Args:
 |      alpha_initializer: Initializer function for the weights.
 |      alpha_regularizer: Regularizer for the weights.
 |      alpha_constraint: Constraint for the weights.
 |      shared_axes: The axes along which to share learnable parameters for the
 |          activation function. For example, if the incoming feature maps are
 |          from a 2D convolution with output shape
 |          `(batch, height, width, channels)`, and you wish to share parameters
 |          across space so that each filter only has one set of parameters,
 |          set `shared_axes=[1, 2]`.
 |      **kwargs: Base layer keyword arguments, such as `name` and `dtype`.
 |
 |  Method resolution order:
 |      PReLU
 |      keras.src.layers.layer.Layer
 |      keras.src.backend.tensorflow.layer.TFLayer
 |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable
 |      tensorflow.python.trackable.autotrackable.AutoTrackable
 |      tensorflow.python.trackable.base.Trackable
 |      keras.src.ops.operation.Operation
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    alpha_initializer='Zeros',
 |    alpha_regularizer=None,
 |    alpha_constraint=None,
 |    shared_axes=None,
 |    **kwargs
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  build(self, input_shape)
 |
 |  call(self, inputs)
 |
 |  compute_output_shape(self, input_shape)
 |
 |  get_config(self)
 |      Returns the config of the object.
 |
 |      An object config is a Python dictionary (serializable)
 |      containing the information needed to re-instantiate it.
 |

