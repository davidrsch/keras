Help on class LogCosh in module keras.src.losses.losses:

class LogCosh(LossFunctionWrapper)
 |  LogCosh(reduction='sum_over_batch_size', name='log_cosh', dtype=None)
 |
 |  Computes the logarithm of the hyperbolic cosine of the prediction error.
 |
 |  Formula:
 |
 |  ```python
 |  error = y_pred - y_true
 |  logcosh = mean(log((exp(error) + exp(-error))/2), axis=-1)`
 |  ```
 |  where x is the error `y_pred - y_true`.
 |
 |  Args:
 |      reduction: Type of reduction to apply to loss. Options are `"sum"`,
 |          `"sum_over_batch_size"` or `None`. Defaults to
 |          `"sum_over_batch_size"`.
 |      name: Optional name for the instance.
 |      dtype: The dtype of the loss's computations. Defaults to `None`, which
 |          means using `keras.backend.floatx()`. `keras.backend.floatx()` is a
 |          `"float32"` unless set to different value
 |          (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is
 |          provided, then the `compute_dtype` will be utilized.
 |
 |  Method resolution order:
 |      LogCosh
 |      LossFunctionWrapper
 |      keras.src.losses.loss.Loss
 |      keras.src.saving.keras_saveable.KerasSaveable
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    reduction='sum_over_batch_size',
 |    name='log_cosh',
 |    dtype=None
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  get_config(self)
 |

