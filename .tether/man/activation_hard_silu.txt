__signature__
keras.activations.hard_silu(x)
__doc__
Hard SiLU activation function, also known as Hard Swish.

It is defined as:

- `0` if `if x < -3`
- `x` if `x > 3`
- `x * (x + 3) / 6` if `-3 <= x <= 3`

It's a faster, piecewise linear approximation of the silu activation.

Args:
    x: Input tensor.

Reference:

- [A Howard, 2019](https://arxiv.org/abs/1905.02244)

