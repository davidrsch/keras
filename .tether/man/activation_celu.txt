__signature__
keras.activations.celu(x, alpha=1.0)
__doc__
Continuously Differentiable Exponential Linear Unit.

The CeLU activation function is defined as:

`celu(x) = alpha * (exp(x / alpha) - 1) for x < 0`,`celu(x) = x for x >= 0`.

where `alpha` is a scaling parameter that controls the activation's shape.

Args:
    x: Input tensor.
    alpha: The Î± value for the CeLU formulation. Defaults to `1.0`.

Reference:

- [Barron, J. T., 2017](https://arxiv.org/abs/1704.07483)

