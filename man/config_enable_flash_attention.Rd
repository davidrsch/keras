% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/config.R
\name{config_enable_flash_attention}
\alias{config_enable_flash_attention}
\title{Enable flash attention.}
\usage{
config_enable_flash_attention()
}
\description{
Flash attention offers performance optimization for attention layers,
making it especially useful for large language models (LLMs) that
benefit from faster and more memory-efficient attention computations.

Once enabled, supported layers like \code{layer_multi_head_attention} will \strong{attempt} to
use flash attention for faster computations. By default, this feature is
enabled.

Note that enabling flash attention does not guarantee it will always be
used. Typically, the inputs must be in \code{float16} or \code{bfloat16} dtype, and
input layout requirements may vary depending on the backend.
}
