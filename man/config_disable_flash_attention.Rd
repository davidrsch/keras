% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/config.R
\name{config_disable_flash_attention}
\alias{config_disable_flash_attention}
\title{Disable flash attention.}
\usage{
config_disable_flash_attention()
}
\description{
Flash attention offers performance optimization for attention layers,
making it especially useful for large language models (LLMs) that
benefit from faster and more memory-efficient attention computations.

Once disabled, supported layers like \code{MultiHeadAttention} will not
use flash attention for faster computations.
}
\seealso{
\itemize{
\item \url{https://www.tensorflow.org/api_docs/python/tf/keras/config/disable_flash_attention}
}
}
