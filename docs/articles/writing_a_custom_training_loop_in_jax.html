<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Writing low-level training &amp; evaluation loops in JAX.">
<title>Writing a training loop from scratch in JAX â€¢ keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Writing a training loop from scratch in JAX">
<meta property="og:description" content="Writing low-level training &amp; evaluation loops in JAX.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/getting_started.html">Getting Started</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a>
    <a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Writing a training loop from scratch in JAX</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes-src/writing_a_custom_training_loop_in_jax.Rmd" class="external-link"><code>vignettes-src/writing_a_custom_training_loop_in_jax.Rmd</code></a></small>
      <div class="d-none name"><code>writing_a_custom_training_loop_in_jax.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># This guide can only be run with the jax backend.</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"jax"</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># We import TF so we can use tf.data.</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Keras provides default training and evaluation loops,
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> and <code><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate()</a></code>. Their usage is covered
in the guide <a href="training_with_built_in_methods.html">Training
&amp; evaluation with the built-in methods</a>.</p>
<p>If you want to customize the learning algorithm of your model while
still leveraging the convenience of <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> (for instance, to
train a GAN using <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>), you can subclass the
<code>Model</code> class and implement your own
<code>train_step()</code> method, which is called repeatedly during
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>.</p>
<p>Now, if you want very low-level control over training &amp;
evaluation, you should write your own training &amp; evaluation loops
from scratch. This is what this guide is about.</p>
</div>
<div class="section level2">
<h2 id="a-first-end-to-end-example">A first end-to-end example<a class="anchor" aria-label="anchor" href="#a-first-end-to-end-example"></a>
</h2>
<p>To write a custom training loop, we need the following
ingredients:</p>
<ul>
<li>A model to train, of course.</li>
<li>An optimizer. You could either use an optimizer from
<code>keras.optimizers</code>, or one from the <code>optax</code>
package.</li>
<li>A loss function.</li>
<li>A dataset. The standard in the JAX ecosystem is to load data via
<code>tf.data</code>, so thatâ€™s what weâ€™ll use.</li>
</ul>
<p>Letâ€™s line them up.</p>
<p>First, letâ€™s get the model and the MNIST dataset:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">def</span> get_model():</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">784</span>,), name<span class="op">=</span><span class="st">"digits"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>    x1 <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    x2 <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x1)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>, name<span class="op">=</span><span class="st">"predictions"</span>)(x2)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>model <span class="op">=</span> get_model()</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># Prepare the training dataset.</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>x_train <span class="op">=</span> np.reshape(x_train, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">784</span>)).astype(<span class="st">"float32"</span>)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>x_test <span class="op">=</span> np.reshape(x_test, (<span class="op">-</span><span class="dv">1</span>, <span class="dv">784</span>)).astype(<span class="st">"float32"</span>)</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>y_train <span class="op">=</span> keras.utils.to_categorical(y_train)</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>y_test <span class="op">=</span> keras.utils.to_categorical(y_test)</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a><span class="co"># Reserve 10,000 samples for validation.</span></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>x_val <span class="op">=</span> x_train[<span class="op">-</span><span class="dv">10000</span>:]</span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>y_val <span class="op">=</span> y_train[<span class="op">-</span><span class="dv">10000</span>:]</span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>x_train <span class="op">=</span> x_train[:<span class="op">-</span><span class="dv">10000</span>]</span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>y_train <span class="op">=</span> y_train[:<span class="op">-</span><span class="dv">10000</span>]</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a><span class="co"># Prepare the training dataset.</span></span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a>train_dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices((x_train, y_train))</span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a>train_dataset <span class="op">=</span> train_dataset.shuffle(buffer_size<span class="op">=</span><span class="dv">1024</span>).batch(batch_size)</span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a><span class="co"># Prepare the validation dataset.</span></span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a>val_dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices((x_val, y_val))</span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a>val_dataset <span class="op">=</span> val_dataset.batch(batch_size)</span></code></pre></div>
<p>Next, hereâ€™s the loss function and the optimizer. Weâ€™ll use a Keras
optimizer in this case.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Instantiate a loss function.</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>loss_fn <span class="op">=</span> keras.losses.CategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Instantiate an optimizer.</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">1e-3</span>)</span></code></pre></div>
<div class="section level3">
<h3 id="getting-gradients-in-jax">Getting gradients in JAX<a class="anchor" aria-label="anchor" href="#getting-gradients-in-jax"></a>
</h3>
<p>Letâ€™s train our model using mini-batch gradient with a custom
training loop.</p>
<p>In JAX, gradients are computed via <em>metaprogramming</em>: you call
the <code>jax.grad</code> (or <code>jax.value_and_grad</code> on a
function in order to create a gradient-computing function for that first
function.</p>
<p>So the first thing we need is a function that returns the loss value.
Thatâ€™s the function weâ€™ll use to generate the gradient function.
Something like this:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">def</span> compute_loss(x, y):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    ...</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code></pre></div>
<p>Once you have such a function, you can compute gradients via
metaprogramming as such:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>grad_fn <span class="op">=</span> jax.grad(compute_loss)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>grads <span class="op">=</span> grad_fn(x, y)</span></code></pre></div>
<p>Typically, you donâ€™t just want to get the gradient values, you also
want to get the loss value. You can do this by using
<code>jax.value_and_grad</code> instead of <code>jax.grad</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>grad_fn <span class="op">=</span> jax.value_and_grad(compute_loss)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>loss, grads <span class="op">=</span> grad_fn(x, y)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="jax-computation-is-purely-stateless">JAX computation is purely stateless<a class="anchor" aria-label="anchor" href="#jax-computation-is-purely-stateless"></a>
</h3>
<p>In JAX, everything must be a stateless function â€“ so our loss
computation function must be stateless as well. That means that all
Keras variables (e.g.Â weight tensors) must be passed as function inputs,
and any variable that has been updated during the forward pass must be
returned as function output. The function have no side effect.</p>
<p>During the forward pass, the non-trainable variables of a Keras model
might get updated. These variables could be, for instance, RNG seed
state variables or BatchNormalization statistics. Weâ€™re going to need to
return those. So we need something like this:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> compute_loss_and_updates(trainable_variables, non_trainable_variables, x, y):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    ...</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    <span class="cf">return</span> loss, non_trainable_variables</span></code></pre></div>
<p>Once you have such a function, you can get the gradient function by
specifying <code>hax_aux</code> in <code>value_and_grad</code>: it tells
JAX that the loss computation function returns more outputs than just
the loss. Note that the loss should always be the first output.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>grad_fn <span class="op">=</span> jax.value_and_grad(compute_loss_and_updates, has_aux<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>(loss, non_trainable_variables), grads <span class="op">=</span> grad_fn(</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    trainable_variables, non_trainable_variables, x, y</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>)</span></code></pre></div>
<p>Now that we have established the basics, letâ€™s implement this
<code>compute_loss_and_updates</code> function. Keras models have a
<code>stateless_call</code> method which will come in handy here. It
works just like <code>model.__call__</code>, but it requires you to
explicitly pass the value of all the variables in the model, and it
returns not just the <code>__call__</code> outputs but also the
(potentially updated) non-trainable variables.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">def</span> compute_loss_and_updates(</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    trainable_variables, non_trainable_variables, x, y</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>):</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    y_pred, non_trainable_variables <span class="op">=</span> model.stateless_call(</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    )</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y, y_pred)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    <span class="cf">return</span> loss, non_trainable_variables</span></code></pre></div>
<p>Letâ€™s get the gradient function:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>grad_fn <span class="op">=</span> jax.value_and_grad(compute_loss_and_updates, has_aux<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="the-training-step-function">The training step function<a class="anchor" aria-label="anchor" href="#the-training-step-function"></a>
</h3>
<p>Next, letâ€™s implement the end-to-end training step, the function that
will both run the forward pass, compute the loss, compute the gradients,
but also use the optimizer to update the trainable variables. This
function also needs to be stateless, so it will get as input a
<code>state</code> tuple that includes every state element weâ€™re going
to use:</p>
<ul>
<li>
<code>trainable_variables</code> and
<code>non_trainable_variables</code>: the modelâ€™s variables.</li>
<li>
<code>optimizer_variables</code>: the optimizerâ€™s state variables,
such as momentum accumulators.</li>
</ul>
<p>To update the trainable variables, we use the optimizerâ€™s stateless
method <code>stateless_apply</code>. Itâ€™s equivalent to
<code>optimizer.apply()</code>, but it requires always passing
<code>trainable_variables</code> and <code>optimizer_variables</code>.
It returns both the updated trainable variables and the updated
optimizer_variables.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">def</span> train_step(state, data):</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    trainable_variables, non_trainable_variables, optimizer_variables <span class="op">=</span> state</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    x, y <span class="op">=</span> data</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    (loss, non_trainable_variables), grads <span class="op">=</span> grad_fn(</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x, y</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    )</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>    trainable_variables, optimizer_variables <span class="op">=</span> optimizer.stateless_apply(</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>        optimizer_variables, grads, trainable_variables</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>    )</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>    <span class="co"># Return updated state</span></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>    <span class="cf">return</span> loss, (</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>        optimizer_variables,</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>    )</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="make-it-fast-with-jax-jit">Make it fast with <code>jax.jit</code><a class="anchor" aria-label="anchor" href="#make-it-fast-with-jax-jit"></a>
</h3>
<p>By default, JAX operations run eagerly, just like in TensorFlow eager
mode and PyTorch eager mode. And just like TensorFlow eager mode and
PyTorch eager mode, itâ€™s pretty slow â€“ eager mode is better used as a
debugging environment, not as a way to do any actual work. So letâ€™s make
our <code>train_step</code> fast by compiling it.</p>
<p>When you have a stateless JAX function, you can compile it to XLA via
the <code>@jax.jit</code> decorator. It will get traced during its first
execution, and in subsequent executions you will be executing the traced
graph (this is just like <code>@tf.function(jit_compile=True)</code>.
Letâ€™s try it:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="kw">def</span> train_step(state, data):</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>    trainable_variables, non_trainable_variables, optimizer_variables <span class="op">=</span> state</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    x, y <span class="op">=</span> data</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    (loss, non_trainable_variables), grads <span class="op">=</span> grad_fn(</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x, y</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>    )</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>    trainable_variables, optimizer_variables <span class="op">=</span> optimizer.stateless_apply(</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>        optimizer_variables, grads, trainable_variables</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>    )</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>    <span class="co"># Return updated state</span></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>    <span class="cf">return</span> loss, (</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>        optimizer_variables,</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>    )</span></code></pre></div>
<p>Weâ€™re now ready to train our model. The training loop itself is
trivial: we just repeatedly call
<code>loss, state = train_step(state, data)</code>.</p>
<p>Note:</p>
<ul>
<li>We convert the TF tensors yielded by the
<code>tf.data.Dataset</code> to NumPy before passing them to our JAX
function.</li>
<li>All variables must be built beforehand: the model must be built and
the optimizer must be built. Since weâ€™re using a Functional API model,
itâ€™s already built, but if it were a subclassed model youâ€™d need to call
it on a batch of data to build it.</li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Build optimizer variables.</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>optimizer.build(model.trainable_variables)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>trainable_variables <span class="op">=</span> model.trainable_variables</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>non_trainable_variables <span class="op">=</span> model.non_trainable_variables</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>optimizer_variables <span class="op">=</span> optimizer.variables</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>state <span class="op">=</span> trainable_variables, non_trainable_variables, optimizer_variables</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a><span class="cf">for</span> step, data <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataset):</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>    data <span class="op">=</span> (data[<span class="dv">0</span>].numpy(), data[<span class="dv">1</span>].numpy())</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>    loss, state <span class="op">=</span> train_step(state, data)</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>    <span class="co"># Log every 100 batches.</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Training loss (for 1 batch) at step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">float</span>(loss)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Seen so far: </span><span class="sc">{</span>(step <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> batch_size<span class="sc">}</span><span class="ss"> samples"</span>)</span></code></pre></div>
<p>A key thing to notice here is that the loop is entirely stateless â€“
the variables attached to the model (<code>model.weights</code>) are
never getting updated during the loop. Their new values are only stored
in the <code>state</code> tuple. That means that at some point, before
saving the model, you should be attaching the new variable values back
to the model.</p>
<p>Just call <code>variable.assign(new_value)</code> on each model
variable you want to update:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>trainable_variables, non_trainable_variables, optimizer_variables <span class="op">=</span> state</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="cf">for</span> variable, value <span class="kw">in</span> <span class="bu">zip</span>(model.trainable_variables, trainable_variables):</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    variable.assign(value)</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="cf">for</span> variable, value <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    model.non_trainable_variables, non_trainable_variables</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>):</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>    variable.assign(value)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="low-level-handling-of-metrics">Low-level handling of metrics<a class="anchor" aria-label="anchor" href="#low-level-handling-of-metrics"></a>
</h2>
<p>Letâ€™s add metrics monitoring to this basic training loop.</p>
<p>You can readily reuse built-in Keras metrics (or custom ones you
wrote) in such training loops written from scratch. Hereâ€™s the flow:</p>
<ul>
<li>Instantiate the metric at the start of the loop</li>
<li>Include <code>metric_variables</code> in the <code>train_step</code>
arguments and <code>compute_loss_and_updates</code> arguments.</li>
<li>Call <code>metric.stateless_update_state()</code> in the
<code>compute_loss_and_updates</code> function. Itâ€™s equivalent to
<code>update_state()</code> â€“ only stateless.</li>
<li>When you need to display the current value of the metric, outside
the <code>train_step</code> (in the eager scope), attach the new metric
variable values to the metric object and vall
<code>metric.result()</code>.</li>
<li>Call <code>metric.reset_state()</code> when you need to clear the
state of the metric (typically at the end of an epoch)</li>
</ul>
<p>Letâ€™s use this knowledge to compute <code>CategoricalAccuracy</code>
on training and validation data at the end of training:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># Get a fresh model</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>model <span class="op">=</span> get_model()</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="co"># Instantiate an optimizer to train the model.</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="co"># Instantiate a loss function.</span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>loss_fn <span class="op">=</span> keras.losses.CategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="co"># Prepare the metrics.</span></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>train_acc_metric <span class="op">=</span> keras.metrics.CategoricalAccuracy()</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>val_acc_metric <span class="op">=</span> keras.metrics.CategoricalAccuracy()</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a><span class="kw">def</span> compute_loss_and_updates(</span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a>    trainable_variables, non_trainable_variables, metric_variables, x, y</span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a>):</span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a>    y_pred, non_trainable_variables <span class="op">=</span> model.stateless_call(</span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x</span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>    )</span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y, y_pred)</span>
<span id="cb15-21"><a href="#cb15-21" tabindex="-1"></a>    metric_variables <span class="op">=</span> train_acc_metric.stateless_update_state(</span>
<span id="cb15-22"><a href="#cb15-22" tabindex="-1"></a>        metric_variables, y, y_pred</span>
<span id="cb15-23"><a href="#cb15-23" tabindex="-1"></a>    )</span>
<span id="cb15-24"><a href="#cb15-24" tabindex="-1"></a>    <span class="cf">return</span> loss, (non_trainable_variables, metric_variables)</span>
<span id="cb15-25"><a href="#cb15-25" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" tabindex="-1"></a>grad_fn <span class="op">=</span> jax.value_and_grad(compute_loss_and_updates, has_aux<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-28"><a href="#cb15-28" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" tabindex="-1"></a></span>
<span id="cb15-30"><a href="#cb15-30" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb15-31"><a href="#cb15-31" tabindex="-1"></a><span class="kw">def</span> train_step(state, data):</span>
<span id="cb15-32"><a href="#cb15-32" tabindex="-1"></a>    (</span>
<span id="cb15-33"><a href="#cb15-33" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb15-34"><a href="#cb15-34" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb15-35"><a href="#cb15-35" tabindex="-1"></a>        optimizer_variables,</span>
<span id="cb15-36"><a href="#cb15-36" tabindex="-1"></a>        metric_variables,</span>
<span id="cb15-37"><a href="#cb15-37" tabindex="-1"></a>    ) <span class="op">=</span> state</span>
<span id="cb15-38"><a href="#cb15-38" tabindex="-1"></a>    x, y <span class="op">=</span> data</span>
<span id="cb15-39"><a href="#cb15-39" tabindex="-1"></a>    (loss, (non_trainable_variables, metric_variables)), grads <span class="op">=</span> grad_fn(</span>
<span id="cb15-40"><a href="#cb15-40" tabindex="-1"></a>        trainable_variables, non_trainable_variables, metric_variables, x, y</span>
<span id="cb15-41"><a href="#cb15-41" tabindex="-1"></a>    )</span>
<span id="cb15-42"><a href="#cb15-42" tabindex="-1"></a>    trainable_variables, optimizer_variables <span class="op">=</span> optimizer.stateless_apply(</span>
<span id="cb15-43"><a href="#cb15-43" tabindex="-1"></a>        optimizer_variables, grads, trainable_variables</span>
<span id="cb15-44"><a href="#cb15-44" tabindex="-1"></a>    )</span>
<span id="cb15-45"><a href="#cb15-45" tabindex="-1"></a>    <span class="co"># Return updated state</span></span>
<span id="cb15-46"><a href="#cb15-46" tabindex="-1"></a>    <span class="cf">return</span> loss, (</span>
<span id="cb15-47"><a href="#cb15-47" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb15-48"><a href="#cb15-48" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb15-49"><a href="#cb15-49" tabindex="-1"></a>        optimizer_variables,</span>
<span id="cb15-50"><a href="#cb15-50" tabindex="-1"></a>        metric_variables,</span>
<span id="cb15-51"><a href="#cb15-51" tabindex="-1"></a>    )</span></code></pre></div>
<p>Weâ€™ll also prepare an evaluation step function:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="kw">def</span> eval_step(state, data):</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    trainable_variables, non_trainable_variables, metric_variables <span class="op">=</span> state</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    x, y <span class="op">=</span> data</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    y_pred, non_trainable_variables <span class="op">=</span> model.stateless_call(</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>    )</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y, y_pred)</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>    metric_variables <span class="op">=</span> val_acc_metric.stateless_update_state(</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>        metric_variables, y, y_pred</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a>    )</span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>    <span class="cf">return</span> loss, (</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>        metric_variables,</span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a>    )</span></code></pre></div>
<p>Here are our loops:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># Build optimizer variables.</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>optimizer.build(model.trainable_variables)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>trainable_variables <span class="op">=</span> model.trainable_variables</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>non_trainable_variables <span class="op">=</span> model.non_trainable_variables</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>optimizer_variables <span class="op">=</span> optimizer.variables</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>metric_variables <span class="op">=</span> train_acc_metric.variables</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>state <span class="op">=</span> (</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>    trainable_variables,</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>    non_trainable_variables,</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>    optimizer_variables,</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    metric_variables,</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>)</span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a><span class="cf">for</span> step, data <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataset):</span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>    data <span class="op">=</span> (data[<span class="dv">0</span>].numpy(), data[<span class="dv">1</span>].numpy())</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>    loss, state <span class="op">=</span> train_step(state, data)</span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a>    <span class="co"># Log every 100 batches.</span></span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Training loss (for 1 batch) at step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">float</span>(loss)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb17-22"><a href="#cb17-22" tabindex="-1"></a>        _, _, _, metric_variables <span class="op">=</span> state</span>
<span id="cb17-23"><a href="#cb17-23" tabindex="-1"></a>        <span class="cf">for</span> variable, value <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb17-24"><a href="#cb17-24" tabindex="-1"></a>            train_acc_metric.variables, metric_variables</span>
<span id="cb17-25"><a href="#cb17-25" tabindex="-1"></a>        ):</span>
<span id="cb17-26"><a href="#cb17-26" tabindex="-1"></a>            variable.assign(value)</span>
<span id="cb17-27"><a href="#cb17-27" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Training accuracy: </span><span class="sc">{</span>train_acc_metric<span class="sc">.</span>result()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-28"><a href="#cb17-28" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Seen so far: </span><span class="sc">{</span>(step <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> batch_size<span class="sc">}</span><span class="ss"> samples"</span>)</span>
<span id="cb17-29"><a href="#cb17-29" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" tabindex="-1"></a>metric_variables <span class="op">=</span> val_acc_metric.variables</span>
<span id="cb17-31"><a href="#cb17-31" tabindex="-1"></a>(</span>
<span id="cb17-32"><a href="#cb17-32" tabindex="-1"></a>    trainable_variables,</span>
<span id="cb17-33"><a href="#cb17-33" tabindex="-1"></a>    non_trainable_variables,</span>
<span id="cb17-34"><a href="#cb17-34" tabindex="-1"></a>    optimizer_variables,</span>
<span id="cb17-35"><a href="#cb17-35" tabindex="-1"></a>    metric_variables,</span>
<span id="cb17-36"><a href="#cb17-36" tabindex="-1"></a>) <span class="op">=</span> state</span>
<span id="cb17-37"><a href="#cb17-37" tabindex="-1"></a>state <span class="op">=</span> trainable_variables, non_trainable_variables, metric_variables</span>
<span id="cb17-38"><a href="#cb17-38" tabindex="-1"></a></span>
<span id="cb17-39"><a href="#cb17-39" tabindex="-1"></a><span class="co"># Eval loop</span></span>
<span id="cb17-40"><a href="#cb17-40" tabindex="-1"></a><span class="cf">for</span> step, data <span class="kw">in</span> <span class="bu">enumerate</span>(val_dataset):</span>
<span id="cb17-41"><a href="#cb17-41" tabindex="-1"></a>    data <span class="op">=</span> (data[<span class="dv">0</span>].numpy(), data[<span class="dv">1</span>].numpy())</span>
<span id="cb17-42"><a href="#cb17-42" tabindex="-1"></a>    loss, state <span class="op">=</span> eval_step(state, data)</span>
<span id="cb17-43"><a href="#cb17-43" tabindex="-1"></a>    <span class="co"># Log every 100 batches.</span></span>
<span id="cb17-44"><a href="#cb17-44" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-45"><a href="#cb17-45" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb17-46"><a href="#cb17-46" tabindex="-1"></a>            <span class="ss">f"Validation loss (for 1 batch) at step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">float</span>(loss)<span class="sc">:.4f}</span><span class="ss">"</span></span>
<span id="cb17-47"><a href="#cb17-47" tabindex="-1"></a>        )</span>
<span id="cb17-48"><a href="#cb17-48" tabindex="-1"></a>        _, _, metric_variables <span class="op">=</span> state</span>
<span id="cb17-49"><a href="#cb17-49" tabindex="-1"></a>        <span class="cf">for</span> variable, value <span class="kw">in</span> <span class="bu">zip</span>(val_acc_metric.variables, metric_variables):</span>
<span id="cb17-50"><a href="#cb17-50" tabindex="-1"></a>            variable.assign(value)</span>
<span id="cb17-51"><a href="#cb17-51" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Validation accuracy: </span><span class="sc">{</span>val_acc_metric<span class="sc">.</span>result()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-52"><a href="#cb17-52" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Seen so far: </span><span class="sc">{</span>(step <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> batch_size<span class="sc">}</span><span class="ss"> samples"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="low-level-handling-of-losses-tracked-by-the-model">Low-level handling of losses tracked by the model<a class="anchor" aria-label="anchor" href="#low-level-handling-of-losses-tracked-by-the-model"></a>
</h2>
<p>Layers &amp; models recursively track any losses created during the
forward pass by layers that call <code>self.add_loss(value)</code>. The
resulting list of scalar loss values are available via the property
<code>model.losses</code> at the end of the forward pass.</p>
<p>If you want to be using these loss components, you should sum them
and add them to the main loss in your training step.</p>
<p>Consider this layer, that creates an activity regularization
loss:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="kw">class</span> ActivityRegularizationLayer(keras.layers.Layer):</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>        <span class="va">self</span>.add_loss(<span class="fl">1e-2</span> <span class="op">*</span> jax.numpy.<span class="bu">sum</span>(inputs))</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>        <span class="cf">return</span> inputs</span></code></pre></div>
<p>Letâ€™s build a really simple model that uses it:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">784</span>,), name<span class="op">=</span><span class="st">"digits"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="co"># Insert activity regularization as a layer</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>x <span class="op">=</span> ActivityRegularizationLayer()(x)</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>, name<span class="op">=</span><span class="st">"predictions"</span>)(x)</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span></code></pre></div>
<p>Hereâ€™s what our <code>compute_loss_and_updates</code> function should
look like now:</p>
<ul>
<li>Pass <code>return_losses=True</code> to
<code>model.stateless_call()</code>.</li>
<li>Sum the resulting <code>losses</code> and add them to the main
loss.</li>
</ul>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="kw">def</span> compute_loss_and_updates(</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>    trainable_variables, non_trainable_variables, metric_variables, x, y</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a>):</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>    y_pred, non_trainable_variables, losses <span class="op">=</span> model.stateless_call(</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x, return_losses<span class="op">=</span><span class="va">True</span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>    )</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y, y_pred)</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a>    <span class="cf">if</span> losses:</span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a>        loss <span class="op">+=</span> jax.numpy.<span class="bu">sum</span>(losses)</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>    metric_variables <span class="op">=</span> train_acc_metric.stateless_update_state(</span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a>        metric_variables, y, y_pred</span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a>    )</span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a>    <span class="cf">return</span> loss, non_trainable_variables, metric_variables</span></code></pre></div>
<p>Thatâ€™s it!</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, FranÃ§ois Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
