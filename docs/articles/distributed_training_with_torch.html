<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Guide to multi-GPU training for Keras models with PyTorch.">
<title>Multi-GPU distributed training with PyTorch • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Multi-GPU distributed training with PyTorch">
<meta property="og:description" content="Guide to multi-GPU training for Keras models with PyTorch.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/getting_started.html">Getting Started</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a>
    <a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Multi-GPU distributed training with PyTorch</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes-src/distributed_training_with_torch.Rmd" class="external-link"><code>vignettes-src/distributed_training_with_torch.Rmd</code></a></small>
      <div class="d-none name"><code>distributed_training_with_torch.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>There are generally two ways to distribute computation across
multiple devices:</p>
<p><strong>Data parallelism</strong>, where a single model gets
replicated on multiple devices or multiple machines. Each of them
processes different batches of data, then they merge their results.
There exist many variants of this setup, that differ in how the
different model replicas merge results, in whether they stay in sync at
every batch or whether they are more loosely coupled, etc.</p>
<p><strong>Model parallelism</strong>, where different parts of a single
model run on different devices, processing a single batch of data
together. This works best with models that have a naturally-parallel
architecture, such as models that feature multiple branches.</p>
<p>This guide focuses on data parallelism, in particular
<strong>synchronous data parallelism</strong>, where the different
replicas of the model stay in sync after each batch they process.
Synchronicity keeps the model convergence behavior identical to what you
would see for single-device training.</p>
<p>Specifically, this guide teaches you how to use PyTorch’s
<code>DistributedDataParallel</code> module wrapper to train Keras, with
minimal changes to your code, on multiple GPUs (typically 2 to 16)
installed on a single machine (single host, multi-device training). This
is the most common setup for researchers and small-scale industry
workflows.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<p>Let’s start by defining the function that creates the model that we
will train, and the function that creates the dataset we will train on
(MNIST in this case).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"torch"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="kw">def</span> get_model():</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>    <span class="co"># Make a simple convnet with batch normalization and dropout.</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Rescaling(<span class="fl">1.0</span> <span class="op">/</span> <span class="fl">255.0</span>)(inputs)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">12</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, use_bias<span class="op">=</span><span class="va">False</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>    )(x)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">24</span>,</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>        use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>        strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>    )(x)</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a>        strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"large_k"</span>,</span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>    )(x)</span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.GlobalAveragePooling2D()(x)</span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a><span class="kw">def</span> get_dataset():</span>
<span id="cb1-45"><a href="#cb1-45" tabindex="-1"></a>    <span class="co"># Load the data and split it between train and test sets</span></span>
<span id="cb1-46"><a href="#cb1-46" tabindex="-1"></a>    (x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb1-47"><a href="#cb1-47" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" tabindex="-1"></a>    <span class="co"># Scale images to the [0, 1] range</span></span>
<span id="cb1-49"><a href="#cb1-49" tabindex="-1"></a>    x_train <span class="op">=</span> x_train.astype(<span class="st">"float32"</span>)</span>
<span id="cb1-50"><a href="#cb1-50" tabindex="-1"></a>    x_test <span class="op">=</span> x_test.astype(<span class="st">"float32"</span>)</span>
<span id="cb1-51"><a href="#cb1-51" tabindex="-1"></a>    <span class="co"># Make sure images have shape (28, 28, 1)</span></span>
<span id="cb1-52"><a href="#cb1-52" tabindex="-1"></a>    x_train <span class="op">=</span> np.expand_dims(x_train, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-53"><a href="#cb1-53" tabindex="-1"></a>    x_test <span class="op">=</span> np.expand_dims(x_test, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-54"><a href="#cb1-54" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"x_train shape:"</span>, x_train.shape)</span>
<span id="cb1-55"><a href="#cb1-55" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" tabindex="-1"></a>    <span class="co"># Create a TensorDataset</span></span>
<span id="cb1-57"><a href="#cb1-57" tabindex="-1"></a>    dataset <span class="op">=</span> torch.utils.data.TensorDataset(</span>
<span id="cb1-58"><a href="#cb1-58" tabindex="-1"></a>        torch.from_numpy(x_train), torch.from_numpy(y_train)</span>
<span id="cb1-59"><a href="#cb1-59" tabindex="-1"></a>    )</span>
<span id="cb1-60"><a href="#cb1-60" tabindex="-1"></a>    <span class="cf">return</span> dataset</span></code></pre></div>
<p>Next, let’s define a simple PyTorch training loop that targets a GPU
(note the calls to <code>.cuda()</code>).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">def</span> train_model(model, dataloader, num_epochs, optimizer, loss_fn):</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>        running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>        running_loss_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>        <span class="cf">for</span> batch_idx, (inputs, targets) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>            inputs <span class="op">=</span> inputs.cuda(non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>            targets <span class="op">=</span> targets.cuda(non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(outputs, targets)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>            <span class="co"># Backward and optimize</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>            loss.backward()</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>            running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>            running_loss_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>        <span class="co"># Print loss statistics</span></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>            <span class="ss">f"Epoch </span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">, "</span></span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>            <span class="ss">f"Loss: </span><span class="sc">{</span>running_loss <span class="op">/</span> running_loss_count<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>        )</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="single-host-multi-device-synchronous-training">Single-host, multi-device synchronous training<a class="anchor" aria-label="anchor" href="#single-host-multi-device-synchronous-training"></a>
</h2>
<p>In this setup, you have one machine with several GPUs on it
(typically 2 to 16). Each device will run a copy of your model (called a
<strong>replica</strong>). For simplicity, in what follows, we’ll assume
we’re dealing with 8 GPUs, at no loss of generality.</p>
<p><strong>How it works</strong></p>
<p>At each step of training:</p>
<ul>
<li>The current batch of data (called <strong>global batch</strong>) is
split into 8 different sub-batches (called <strong>local
batches</strong>). For instance, if the global batch has 512 samples,
each of the 8 local batches will have 64 samples.</li>
<li>Each of the 8 replicas independently processes a local batch: they
run a forward pass, then a backward pass, outputting the gradient of the
weights with respect to the loss of the model on the local batch.</li>
<li>The weight updates originating from local gradients are efficiently
merged across the 8 replicas. Because this is done at the end of every
step, the replicas always stay in sync.</li>
</ul>
<p>In practice, the process of synchronously updating the weights of the
model replicas is handled at the level of each individual weight
variable. This is done through a <strong>mirrored variable</strong>
object.</p>
<p><strong>How to use it</strong></p>
<p>To do single-host, multi-device synchronous training with a Keras
model, you would use the
<code>torch.nn.parallel.DistributedDataParallel</code> module wrapper.
Here’s how it works:</p>
<ul>
<li>We use <code>torch.multiprocessing.start_processes</code> to start
multiple Python processes, one per device. Each process will run the
<code>per_device_launch_fn</code> function.</li>
<li>The <code>per_device_launch_fn</code> function does the following:
<ul>
<li>It uses <code>torch.distributed.init_process_group</code> and
<code>torch.cuda.set_device</code> to configure the device to be used
for that process.</li>
<li>It uses <code>torch.utils.data.distributed.DistributedSampler</code>
and <code>torch.utils.data.DataLoader</code> to turn our data into a
distributed data loader.</li>
<li>It also uses <code>torch.nn.parallel.DistributedDataParallel</code>
to turn our model into a distributed PyTorch module.</li>
<li>It then calls the <code>train_model</code> function.</li>
</ul>
</li>
<li>The <code>train_model</code> function will then run in each process,
with the model using a separate device in each process.</li>
</ul>
<p>Here’s the flow, where each step is split into its own utility
function:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Config</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>num_gpu <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Running on </span><span class="sc">{</span>num_gpu<span class="sc">}</span><span class="ss"> GPUs"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="kw">def</span> setup_device(current_gpu_index, num_gpus):</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    <span class="co"># Device setup</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_ADDR"</span>] <span class="op">=</span> <span class="st">"localhost"</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    os.environ[<span class="st">"MASTER_PORT"</span>] <span class="op">=</span> <span class="st">"56492"</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda:</span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(current_gpu_index))</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    torch.distributed.init_process_group(</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">"nccl"</span>,</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>        init_method<span class="op">=</span><span class="st">"env://"</span>,</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>        world_size<span class="op">=</span>num_gpus,</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>        rank<span class="op">=</span>current_gpu_index,</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>    )</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>    torch.cuda.set_device(device)</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a><span class="kw">def</span> cleanup():</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>    torch.distributed.destroy_process_group()</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a><span class="kw">def</span> prepare_dataloader(dataset, current_gpu_index, num_gpus, batch_size):</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>    sampler <span class="op">=</span> torch.utils.data.distributed.DistributedSampler(</span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>        dataset,</span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>        num_replicas<span class="op">=</span>num_gpus,</span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>        rank<span class="op">=</span>current_gpu_index,</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>    )</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a>    dataloader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>        dataset,</span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>        sampler<span class="op">=</span>sampler,</span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a>    )</span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a>    <span class="cf">return</span> dataloader</span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a><span class="kw">def</span> per_device_launch_fn(current_gpu_index, num_gpu):</span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a>    <span class="co"># Setup the process groups</span></span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a>    setup_device(current_gpu_index, num_gpu)</span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a>    dataset <span class="op">=</span> get_dataset()</span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a>    model <span class="op">=</span> get_model()</span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a>    <span class="co"># prepare the dataloader</span></span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a>    dataloader <span class="op">=</span> prepare_dataloader(</span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a>        dataset, current_gpu_index, num_gpu, batch_size</span>
<span id="cb3-52"><a href="#cb3-52" tabindex="-1"></a>    )</span>
<span id="cb3-53"><a href="#cb3-53" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" tabindex="-1"></a>    <span class="co"># Instantiate the torch optimizer</span></span>
<span id="cb3-55"><a href="#cb3-55" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb3-56"><a href="#cb3-56" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" tabindex="-1"></a>    <span class="co"># Instantiate the torch loss function</span></span>
<span id="cb3-58"><a href="#cb3-58" tabindex="-1"></a>    loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb3-59"><a href="#cb3-59" tabindex="-1"></a></span>
<span id="cb3-60"><a href="#cb3-60" tabindex="-1"></a>    <span class="co"># Put model on device</span></span>
<span id="cb3-61"><a href="#cb3-61" tabindex="-1"></a>    model <span class="op">=</span> model.to(current_gpu_index)</span>
<span id="cb3-62"><a href="#cb3-62" tabindex="-1"></a>    ddp_model <span class="op">=</span> torch.nn.parallel.DistributedDataParallel(</span>
<span id="cb3-63"><a href="#cb3-63" tabindex="-1"></a>        model, device_ids<span class="op">=</span>[current_gpu_index], output_device<span class="op">=</span>current_gpu_index</span>
<span id="cb3-64"><a href="#cb3-64" tabindex="-1"></a>    )</span>
<span id="cb3-65"><a href="#cb3-65" tabindex="-1"></a></span>
<span id="cb3-66"><a href="#cb3-66" tabindex="-1"></a>    train_model(ddp_model, dataloader, num_epochs, optimizer, loss_fn)</span>
<span id="cb3-67"><a href="#cb3-67" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" tabindex="-1"></a>    cleanup()</span></code></pre></div>
<p>Time to start multiple processes:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="co"># We use the "fork" method rather than "spawn" to support notebooks</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    torch.multiprocessing.start_processes(</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        per_device_launch_fn,</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        args<span class="op">=</span>(num_gpu,),</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        nprocs<span class="op">=</span>num_gpu,</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        join<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>        start_method<span class="op">=</span><span class="st">"fork"</span>,</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    )</span></code></pre></div>
<p>That’s it!</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
