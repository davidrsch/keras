<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Guide to multi-GPU/TPU training for Keras models with JAX.">
<title>Multi-GPU distributed training with JAX • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Multi-GPU distributed training with JAX">
<meta property="og:description" content="Guide to multi-GPU/TPU training for Keras models with JAX.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/getting_started.html">Getting Started</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a>
    <a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Multi-GPU distributed training with JAX</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes-src/distributed_training_with_jax.Rmd" class="external-link"><code>vignettes-src/distributed_training_with_jax.Rmd</code></a></small>
      <div class="d-none name"><code>distributed_training_with_jax.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>There are generally two ways to distribute computation across
multiple devices:</p>
<p><strong>Data parallelism</strong>, where a single model gets
replicated on multiple devices or multiple machines. Each of them
processes different batches of data, then they merge their results.
There exist many variants of this setup, that differ in how the
different model replicas merge results, in whether they stay in sync at
every batch or whether they are more loosely coupled, etc.</p>
<p><strong>Model parallelism</strong>, where different parts of a single
model run on different devices, processing a single batch of data
together. This works best with models that have a naturally-parallel
architecture, such as models that feature multiple branches.</p>
<p>This guide focuses on data parallelism, in particular
<strong>synchronous data parallelism</strong>, where the different
replicas of the model stay in sync after each batch they process.
Synchronicity keeps the model convergence behavior identical to what you
would see for single-device training.</p>
<p>Specifically, this guide teaches you how to use
<code>jax.sharding</code> APIs to train Keras models, with minimal
changes to your code, on multiple GPUs or TPUS (typically 2 to 16)
installed on a single machine (single host, multi-device training). This
is the most common setup for researchers and small-scale industry
workflows.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<p>Let’s start by defining the function that creates the model that we
will train, and the function that creates the dataset we will train on
(MNIST in this case).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"jax"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> jax.experimental <span class="im">import</span> mesh_utils</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">from</span> jax.sharding <span class="im">import</span> Mesh</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">from</span> jax.sharding <span class="im">import</span> NamedSharding</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="im">from</span> jax.sharding <span class="im">import</span> PartitionSpec <span class="im">as</span> P</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="kw">def</span> get_model():</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>    <span class="co"># Make a simple convnet with batch normalization and dropout.</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>))</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Rescaling(<span class="fl">1.0</span> <span class="op">/</span> <span class="fl">255.0</span>)(inputs)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">12</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, use_bias<span class="op">=</span><span class="va">False</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>    )(x)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">24</span>,</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a>        use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>        strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>    )(x)</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Conv2D(</span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a>        kernel_size<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"same"</span>,</span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a>        strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"large_k"</span>,</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a>    )(x)</span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.BatchNormalization(scale<span class="op">=</span><span class="va">False</span>, center<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.ReLU()(x)</span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.GlobalAveragePooling2D()(x)</span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a>    x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb1-45"><a href="#cb1-45" tabindex="-1"></a>    outputs <span class="op">=</span> keras.layers.Dense(<span class="dv">10</span>)(x)</span>
<span id="cb1-46"><a href="#cb1-46" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb1-47"><a href="#cb1-47" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb1-48"><a href="#cb1-48" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" tabindex="-1"></a><span class="kw">def</span> get_datasets():</span>
<span id="cb1-51"><a href="#cb1-51" tabindex="-1"></a>    <span class="co"># Load the data and split it between train and test sets</span></span>
<span id="cb1-52"><a href="#cb1-52" tabindex="-1"></a>    (x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb1-53"><a href="#cb1-53" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" tabindex="-1"></a>    <span class="co"># Scale images to the [0, 1] range</span></span>
<span id="cb1-55"><a href="#cb1-55" tabindex="-1"></a>    x_train <span class="op">=</span> x_train.astype(<span class="st">"float32"</span>)</span>
<span id="cb1-56"><a href="#cb1-56" tabindex="-1"></a>    x_test <span class="op">=</span> x_test.astype(<span class="st">"float32"</span>)</span>
<span id="cb1-57"><a href="#cb1-57" tabindex="-1"></a>    <span class="co"># Make sure images have shape (28, 28, 1)</span></span>
<span id="cb1-58"><a href="#cb1-58" tabindex="-1"></a>    x_train <span class="op">=</span> np.expand_dims(x_train, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-59"><a href="#cb1-59" tabindex="-1"></a>    x_test <span class="op">=</span> np.expand_dims(x_test, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-60"><a href="#cb1-60" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"x_train shape:"</span>, x_train.shape)</span>
<span id="cb1-61"><a href="#cb1-61" tabindex="-1"></a>    <span class="bu">print</span>(x_train.shape[<span class="dv">0</span>], <span class="st">"train samples"</span>)</span>
<span id="cb1-62"><a href="#cb1-62" tabindex="-1"></a>    <span class="bu">print</span>(x_test.shape[<span class="dv">0</span>], <span class="st">"test samples"</span>)</span>
<span id="cb1-63"><a href="#cb1-63" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" tabindex="-1"></a>    <span class="co"># Create TF Datasets</span></span>
<span id="cb1-65"><a href="#cb1-65" tabindex="-1"></a>    train_data <span class="op">=</span> tf.data.Dataset.from_tensor_slices((x_train, y_train))</span>
<span id="cb1-66"><a href="#cb1-66" tabindex="-1"></a>    eval_data <span class="op">=</span> tf.data.Dataset.from_tensor_slices((x_test, y_test))</span>
<span id="cb1-67"><a href="#cb1-67" tabindex="-1"></a>    <span class="cf">return</span> train_data, eval_data</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="single-host-multi-device-synchronous-training">Single-host, multi-device synchronous training<a class="anchor" aria-label="anchor" href="#single-host-multi-device-synchronous-training"></a>
</h2>
<p>In this setup, you have one machine with several GPUs or TPUs on it
(typically 2 to 16). Each device will run a copy of your model (called a
<strong>replica</strong>). For simplicity, in what follows, we’ll assume
we’re dealing with 8 GPUs, at no loss of generality.</p>
<p><strong>How it works</strong></p>
<p>At each step of training:</p>
<ul>
<li>The current batch of data (called <strong>global batch</strong>) is
split into 8 different sub-batches (called <strong>local
batches</strong>). For instance, if the global batch has 512 samples,
each of the 8 local batches will have 64 samples.</li>
<li>Each of the 8 replicas independently processes a local batch: they
run a forward pass, then a backward pass, outputting the gradient of the
weights with respect to the loss of the model on the local batch.</li>
<li>The weight updates originating from local gradients are efficiently
merged across the 8 replicas. Because this is done at the end of every
step, the replicas always stay in sync.</li>
</ul>
<p>In practice, the process of synchronously updating the weights of the
model replicas is handled at the level of each individual weight
variable. This is done through a using a
<code>jax.sharding.NamedSharding</code> that is configured to replicate
the variables.</p>
<p><strong>How to use it</strong></p>
<p>To do single-host, multi-device synchronous training with a Keras
model, you would use the <code>jax.sharding</code> features. Here’s how
it works:</p>
<ul>
<li>We first create a device mesh using
<code>mesh_utils.create_device_mesh</code>.</li>
<li>We use <code>jax.sharding.Mesh</code>,
<code>jax.sharding.NamedSharding</code> and
<code>jax.sharding.PartitionSpec</code> to define how to partition JAX
arrays.
<ul>
<li>We specify that we want to replicate the model and optimizer
variables across all devices by using a spec with no axis.</li>
<li>We specify that we want to shard the data across devices by using a
spec that splits along the batch dimension.</li>
</ul>
</li>
<li>We use <code>jax.device_put</code> to replicate the model and
optimizer variables across devices. This happens once at the
beginning.</li>
<li>In the training loop, for each batch that we process, we use
<code>jax.device_put</code> to split the batch across devices before
invoking the train step.</li>
</ul>
<p>Here’s the flow, where each step is split into its own utility
function:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Config</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>train_data, eval_data <span class="op">=</span> get_datasets()</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>train_data <span class="op">=</span> train_data.batch(batch_size, drop_remainder<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>model <span class="op">=</span> get_model()</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(<span class="fl">1e-3</span>)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># Initialize all state with .build()</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>(one_batch, one_batch_labels) <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_data))</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>model.build(one_batch)</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>optimizer.build(model.trainable_variables)</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="co"># This is the loss function that will be differentiated.</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co"># Keras provides a pure functional forward pass: model.stateless_call</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a><span class="kw">def</span> compute_loss(trainable_variables, non_trainable_variables, x, y):</span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>    y_pred, updated_non_trainable_variables <span class="op">=</span> model.stateless_call(</span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x</span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>    )</span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>    loss_value <span class="op">=</span> loss(y, y_pred)</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>    <span class="cf">return</span> loss_value, updated_non_trainable_variables</span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a><span class="co"># Function to compute gradients</span></span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a>compute_gradients <span class="op">=</span> jax.value_and_grad(compute_loss, has_aux<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a><span class="co"># Training step, Keras provides a pure functional optimizer.stateless_apply</span></span>
<span id="cb2-33"><a href="#cb2-33" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb2-34"><a href="#cb2-34" tabindex="-1"></a><span class="kw">def</span> train_step(train_state, x, y):</span>
<span id="cb2-35"><a href="#cb2-35" tabindex="-1"></a>    (</span>
<span id="cb2-36"><a href="#cb2-36" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb2-37"><a href="#cb2-37" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb2-38"><a href="#cb2-38" tabindex="-1"></a>        optimizer_variables,</span>
<span id="cb2-39"><a href="#cb2-39" tabindex="-1"></a>    ) <span class="op">=</span> train_state</span>
<span id="cb2-40"><a href="#cb2-40" tabindex="-1"></a>    (loss_value, non_trainable_variables), grads <span class="op">=</span> compute_gradients(</span>
<span id="cb2-41"><a href="#cb2-41" tabindex="-1"></a>        trainable_variables, non_trainable_variables, x, y</span>
<span id="cb2-42"><a href="#cb2-42" tabindex="-1"></a>    )</span>
<span id="cb2-43"><a href="#cb2-43" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" tabindex="-1"></a>    trainable_variables, optimizer_variables <span class="op">=</span> optimizer.stateless_apply(</span>
<span id="cb2-45"><a href="#cb2-45" tabindex="-1"></a>        optimizer_variables, grads, trainable_variables</span>
<span id="cb2-46"><a href="#cb2-46" tabindex="-1"></a>    )</span>
<span id="cb2-47"><a href="#cb2-47" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" tabindex="-1"></a>    <span class="cf">return</span> loss_value, (</span>
<span id="cb2-49"><a href="#cb2-49" tabindex="-1"></a>        trainable_variables,</span>
<span id="cb2-50"><a href="#cb2-50" tabindex="-1"></a>        non_trainable_variables,</span>
<span id="cb2-51"><a href="#cb2-51" tabindex="-1"></a>        optimizer_variables,</span>
<span id="cb2-52"><a href="#cb2-52" tabindex="-1"></a>    )</span>
<span id="cb2-53"><a href="#cb2-53" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" tabindex="-1"></a><span class="co"># Replicate the model and optimizer variable on all devices</span></span>
<span id="cb2-56"><a href="#cb2-56" tabindex="-1"></a><span class="kw">def</span> get_replicated_train_state(devices):</span>
<span id="cb2-57"><a href="#cb2-57" tabindex="-1"></a>    <span class="co"># All variables will be replicated on all devices</span></span>
<span id="cb2-58"><a href="#cb2-58" tabindex="-1"></a>    var_mesh <span class="op">=</span> Mesh(devices, axis_names<span class="op">=</span>(<span class="st">"_"</span>))</span>
<span id="cb2-59"><a href="#cb2-59" tabindex="-1"></a>    <span class="co"># In NamedSharding, axes not mentioned are replicated (all axes here)</span></span>
<span id="cb2-60"><a href="#cb2-60" tabindex="-1"></a>    var_replication <span class="op">=</span> NamedSharding(var_mesh, P())</span>
<span id="cb2-61"><a href="#cb2-61" tabindex="-1"></a></span>
<span id="cb2-62"><a href="#cb2-62" tabindex="-1"></a>    <span class="co"># Apply the distribution settings to the model variables</span></span>
<span id="cb2-63"><a href="#cb2-63" tabindex="-1"></a>    trainable_variables <span class="op">=</span> jax.device_put(</span>
<span id="cb2-64"><a href="#cb2-64" tabindex="-1"></a>        model.trainable_variables, var_replication</span>
<span id="cb2-65"><a href="#cb2-65" tabindex="-1"></a>    )</span>
<span id="cb2-66"><a href="#cb2-66" tabindex="-1"></a>    non_trainable_variables <span class="op">=</span> jax.device_put(</span>
<span id="cb2-67"><a href="#cb2-67" tabindex="-1"></a>        model.non_trainable_variables, var_replication</span>
<span id="cb2-68"><a href="#cb2-68" tabindex="-1"></a>    )</span>
<span id="cb2-69"><a href="#cb2-69" tabindex="-1"></a>    optimizer_variables <span class="op">=</span> jax.device_put(optimizer.variables, var_replication)</span>
<span id="cb2-70"><a href="#cb2-70" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" tabindex="-1"></a>    <span class="co"># Combine all state in a tuple</span></span>
<span id="cb2-72"><a href="#cb2-72" tabindex="-1"></a>    <span class="cf">return</span> (trainable_variables, non_trainable_variables, optimizer_variables)</span>
<span id="cb2-73"><a href="#cb2-73" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" tabindex="-1"></a>num_devices <span class="op">=</span> <span class="bu">len</span>(jax.local_devices())</span>
<span id="cb2-76"><a href="#cb2-76" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Running on </span><span class="sc">{</span>num_devices<span class="sc">}</span><span class="ss"> devices: </span><span class="sc">{</span>jax<span class="sc">.</span>local_devices()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-77"><a href="#cb2-77" tabindex="-1"></a>devices <span class="op">=</span> mesh_utils.create_device_mesh((num_devices,))</span>
<span id="cb2-78"><a href="#cb2-78" tabindex="-1"></a></span>
<span id="cb2-79"><a href="#cb2-79" tabindex="-1"></a><span class="co"># Data will be split along the batch axis</span></span>
<span id="cb2-80"><a href="#cb2-80" tabindex="-1"></a>data_mesh <span class="op">=</span> Mesh(devices, axis_names<span class="op">=</span>(<span class="st">"batch"</span>,))  <span class="co"># naming axes of the mesh</span></span>
<span id="cb2-81"><a href="#cb2-81" tabindex="-1"></a>data_sharding <span class="op">=</span> NamedSharding(</span>
<span id="cb2-82"><a href="#cb2-82" tabindex="-1"></a>    data_mesh,</span>
<span id="cb2-83"><a href="#cb2-83" tabindex="-1"></a>    P(</span>
<span id="cb2-84"><a href="#cb2-84" tabindex="-1"></a>        <span class="st">"batch"</span>,</span>
<span id="cb2-85"><a href="#cb2-85" tabindex="-1"></a>    ),</span>
<span id="cb2-86"><a href="#cb2-86" tabindex="-1"></a>)  <span class="co"># naming axes of the sharded partition</span></span>
<span id="cb2-87"><a href="#cb2-87" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" tabindex="-1"></a><span class="co"># Display data sharding</span></span>
<span id="cb2-89"><a href="#cb2-89" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_data))</span>
<span id="cb2-90"><a href="#cb2-90" tabindex="-1"></a>sharded_x <span class="op">=</span> jax.device_put(x.numpy(), data_sharding)</span>
<span id="cb2-91"><a href="#cb2-91" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Data sharding"</span>)</span>
<span id="cb2-92"><a href="#cb2-92" tabindex="-1"></a>jax.debug.visualize_array_sharding(jax.numpy.reshape(sharded_x, [<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>]))</span>
<span id="cb2-93"><a href="#cb2-93" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" tabindex="-1"></a>train_state <span class="op">=</span> get_replicated_train_state(devices)</span>
<span id="cb2-95"><a href="#cb2-95" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" tabindex="-1"></a><span class="co"># Custom training loop</span></span>
<span id="cb2-97"><a href="#cb2-97" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb2-98"><a href="#cb2-98" tabindex="-1"></a>    data_iter <span class="op">=</span> <span class="bu">iter</span>(train_data)</span>
<span id="cb2-99"><a href="#cb2-99" tabindex="-1"></a>    <span class="cf">for</span> data <span class="kw">in</span> data_iter:</span>
<span id="cb2-100"><a href="#cb2-100" tabindex="-1"></a>        x, y <span class="op">=</span> data</span>
<span id="cb2-101"><a href="#cb2-101" tabindex="-1"></a>        sharded_x <span class="op">=</span> jax.device_put(x.numpy(), data_sharding)</span>
<span id="cb2-102"><a href="#cb2-102" tabindex="-1"></a>        loss_value, train_state <span class="op">=</span> train_step(train_state, sharded_x, y.numpy())</span>
<span id="cb2-103"><a href="#cb2-103" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Epoch"</span>, epoch, <span class="st">"loss:"</span>, loss_value)</span>
<span id="cb2-104"><a href="#cb2-104" tabindex="-1"></a></span>
<span id="cb2-105"><a href="#cb2-105" tabindex="-1"></a><span class="co"># Post-processing model state update to write them back into the model</span></span>
<span id="cb2-106"><a href="#cb2-106" tabindex="-1"></a>trainable_variables, non_trainable_variables, optimizer_variables <span class="op">=</span> train_state</span>
<span id="cb2-107"><a href="#cb2-107" tabindex="-1"></a><span class="cf">for</span> variable, value <span class="kw">in</span> <span class="bu">zip</span>(model.trainable_variables, trainable_variables):</span>
<span id="cb2-108"><a href="#cb2-108" tabindex="-1"></a>    variable.assign(value)</span>
<span id="cb2-109"><a href="#cb2-109" tabindex="-1"></a><span class="cf">for</span> variable, value <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb2-110"><a href="#cb2-110" tabindex="-1"></a>    model.non_trainable_variables, non_trainable_variables</span>
<span id="cb2-111"><a href="#cb2-111" tabindex="-1"></a>):</span>
<span id="cb2-112"><a href="#cb2-112" tabindex="-1"></a>    variable.assign(value)</span></code></pre></div>
<p>That’s it!</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
